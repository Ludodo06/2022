{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": "true"
   },
   "source": [
    "<a id=\"0\"></a> <br>\n",
    " # Table of Contents  \n",
    "0. [Miscalleneous](#1)     \n",
    "1. [Handling Data](#2) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General\n",
    "### Download files from GitHub\n",
    "Download a folder (repository): https://download-directory.github.io/ \\\n",
    "Download a single file: Click on file → Right click on « Raw » button → Save as\n",
    "### Get information about function inputs/outputs:\n",
    "Use \"?\". Also gives some instructions. Example: plt.scatter?\n",
    "\n",
    "### Format for prints\n",
    "- Use %d for integers, %f for floats (%.xf to specify a precision of x), %s for strings \n",
    "print(\"The mean is %f apples and %f oranges.\" % (apples, oranges)) \n",
    "- Use .format(), no need to specify data type \\\n",
    "print('The mean is {} apples and {} oranges'.format(apples, oranges))\n",
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Miscalleneous <a id=\"1\"> </a> \n",
    "## 0.1 - import libraries \n",
    "Install new libraries directly from Notebook: ! pip install library_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# Statistics\\nfrom scipy import stats\\nimport statsmodels.api as sm\\nimport statsmodels.formula.api as smf\\nfrom statsmodels.stats import diagnostic\\n# Machine learning (regression, classification, dimensioanlity reduction, split, performance...)\\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, LogisticRegressionCV\\nfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.model_selection import cross_val_predict\\nfrom sklearn.model_selection import cross_val_score\\nfrom sklearn.metrics import mean_squared_error, auc, roc_curve\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.cluster import KMeans, DBSCAN\\nfrom sklearn.metrics import silhouette_score\\nfrom sklearn.manifold import TSNE\\nfrom sklearn.decomposition import PCA\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.decomposition import TruncatedSVD\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, balanced\\n# Networks\\nimport networkx as nx\\nfrom networkx.algorithms.community.centrality import girvan_newman\\nfrom community import community_louvain\\n0.2 - Loading data in dataframes\\nSeparator type\\nDefault is comma ',' , tab '\\t' , semicolon ';' , vertical bar '|' , colon ':'\\nDecimal data with commas\\nExample 12,6 instead of 12.6: add in the parenthesis decimal=','\\nImport only first 10 rows\\nAdd in the parenthesis nrows=10\\n0.3 - Preview dataframe\\n0.4 - Get list of row/column names\\n1 - Handling data\\n1.1 - Merge dataframes\\n1.2 - Drop columns / rows\\naxis=0 will act on all the ROWS, in each column\\naxis=1 will act on all the COLUMNS, in each row\\nfrom operator import itemgetter\\nimport collections\\n# Turn off notebook warnings\\nimport warnings\\nwarnings.filterwarnings('ignore')\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import json\n",
    "import bz2\n",
    "\n",
    "# Statistics\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.stats import diagnostic\n",
    "'''\n",
    "# Machine learning (regression, classification, dimensioanlity reduction, split, performance...)\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, LogisticRegressionCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, auc, roc_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, average_precision_score, balanced'''\n",
    "# Networks\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "from community import community_louvain\n",
    "'''\n",
    "0.2 - Loading data in dataframes\n",
    "Separator type\n",
    "Default is comma ',' , tab '\\t' , semicolon ';' , vertical bar '|' , colon ':'\n",
    "Decimal data with commas\n",
    "Example 12,6 instead of 12.6: add in the parenthesis decimal=','\n",
    "Import only first 10 rows\n",
    "Add in the parenthesis nrows=10\n",
    "0.3 - Preview dataframe\n",
    "0.4 - Get list of row/column names\n",
    "1 - Handling data\n",
    "1.1 - Merge dataframes\n",
    "1.2 - Drop columns / rows\n",
    "axis=0 will act on all the ROWS, in each column\n",
    "axis=1 will act on all the COLUMNS, in each row\n",
    "from operator import itemgetter\n",
    "import collections\n",
    "# Turn off notebook warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 - Loading Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/file.csv', sep=',')\n",
    "# pd.read_csv?\n",
    "df = pd.read_json('Data/file.json')\n",
    "# pd.read_json?\n",
    "df = pd.read_excel('Data/file.xlsx')\n",
    "# pd.read_excel?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separator type\n",
    "Default is comma ',' , tab '\\t' , semicolon ';' , vertical bar '|' , colon ':'\n",
    "### Decimal data with commas\n",
    "Example 12,6 instead of 12.6: add in the parenthesis decimal=','\n",
    "### Import only first 10 rows\n",
    "Add in the parenthesis nrows=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 - preview Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)\n",
    "df.tail(5)\n",
    "df.sample(5)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.4 - list of row and column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of row names (index):\n",
    "list(df.index)\n",
    "# Get list of column names:\n",
    "list(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 - Writing Data to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"mb.csv\")\n",
    "df.to_pickle(\"baseball_pickle\") # An efficient way of storing data to disk is in **binary format**. Pandas supports this using Python’s built-in pickle serialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Handling Data <a id=\"2\"> </a> \n",
    "## 1.1 Merging DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging two dataframes df1, df2 on common column\n",
    "df = df1.merge(df2, on='common_column')\n",
    "\n",
    "# By default, inner join (intersection of the two dfs).\n",
    "# If a row from df1 is missing in df2, it will be discarded => loss of info\n",
    "# If you dont want this, do outer join (will add NaN in columns from df2 that did not appear):\n",
    "df = pd.merge(df1, df2, how='outer')\n",
    "\n",
    "# ----- Concatenate -----\n",
    "# Axis=0 is default, will obtain df with the the rows concatenated\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "\n",
    "# Axis=1 will concatenate column-wise, but respecting the row indices of the two dfs.\n",
    "df = pd.concat([df1, df2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 selecting some columns of dataframe \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['col1','col2',...,'col(n)']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 dropping column and rows \n",
    "- axis=0 will act on all the ROWS, in each column\n",
    "- axis=1 will act on all the COLUMNS, in each r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"drop_this_column\", axis=1)\n",
    "df.drop(\"drop_this_row\", axis=0)\n",
    "# Drop rows with at least one missing value (NaN)\n",
    "df.dropna()\n",
    "# Drop rows with all elements that are missing values\n",
    "df.dropna(how='all')\n",
    "# Drop duplicates based on id, keeping first\n",
    "df.drop_duplicates(inplace=True, subset=['id'], keep='first')\n",
    "# Drop duplicates based on two columns (two rows with albums by the same band with the same name)\n",
    "df.drop_duplicates(inplace=True, subset=['artist','album'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Reorganizing data: sort / group by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort rows in increasing/decreasing order according to a column\n",
    "df.sort_values(\"according_to_this_column\", ascending=False)\n",
    "# Count of unique values in column\n",
    "df.column1.value_counts()\n",
    "# Group rows by same column value\n",
    "df = df.groupyby(\"this_column\")\n",
    "# Group rows by same column value and take mean of other columns in doing so\n",
    "df = df.groupby(['this_column']).mean()\n",
    "# Group rows by same column and take total count from other columns while doing so\n",
    "df = df.groupby(['this_column']).count()\n",
    "# Typically need to perform a reset index after a groupby\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 - Isolate/select specific data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple way to choose only the rows where the team column value is \"Boston Celtics\":\n",
    "df[df.Team=='Boston Celtics']\n",
    "# For several conditions:\n",
    "df[(df.Team=='Boston Celtics') & (df.Year==2006)] # AND condition\n",
    "df[(df.Team=='Boston Celtics') | (df.Year==2006)] # OR condition\n",
    "# For several possibilities, in same column\n",
    "df[(df.Team=='Boston Celtics') | (df.Team=='Tornadoes')]\n",
    "df[df.Team.isin(['Boston Celtics', 'Tornadoes'])]\n",
    "# ----- INDEXING ------\n",
    "# Select row position by index name:\n",
    "df.loc['row_name']\n",
    "# Select row position by absolute index:\n",
    "df.iloc[i]\n",
    "# Select data based on dtypes\n",
    "float_df = df.select_dtypes(include=['float64'])\n",
    "# Select all columns that contain a specific word in name\n",
    "# Example: Select only the {x}_onehot columns\n",
    "onehot_cols = [col for col in df.columns if 'onehot' in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 - replacing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a certain column, replace Old1, Old2 and Old3 by New1, New2 and New3 respectively\n",
    "df.Column_Name.replace({'Old1':'New1', 'Old2':'New2', 'Old3':'New3'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 - Get unique values in column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of unique values in a column:\n",
    "df.Column1.nunique()\n",
    "# Get all the unique values in a column:\n",
    "df.Column1.unique()\n",
    "# Get a count of number of elements for all unique values in column\n",
    "df.topic.value_counts()\n",
    "# If you have to do this for all columns:\n",
    "column = list(df.columns)\n",
    "for i in range(len(column)):\n",
    "df[column[i]].nunique()\n",
    "df[column[i]].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 - Apply function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One line method\n",
    "data_features['adopted'] = data_features['outcome_type'].apply(lambda r: 1 if r=='Adoption' else 0)\n",
    "\n",
    "# More detailed method, equivalent\n",
    "def funct(r):\n",
    "if r=='Adoption': return 1\n",
    "else: return 0\n",
    "data_features['adopted'] = data_features['outcome_type'].apply(funct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.8 - date time conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column to datetime format\n",
    "df.date = pd.to_datetime(df.date)\n",
    "# Remove hours\n",
    "df['date'] = df['date'].dt.date\n",
    "# Get current datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.9 - get the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['col_name'].values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Vizualizing Data <a id=\"3\"> </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 -  simple plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TIP: Use the one from pandas if you need to be fast, labels axes automaticaly, figsize easier\n",
    "\n",
    " # Using PANDAS directly\n",
    "df.plot(x='column1', y='column2', kind='scatter', title='title', figsize=(6,4));\n",
    "\n",
    "# other kinds: 'line', 'bar', 'barh', 'hist', 'box', 'pie', 'scatter'\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Using MATPLOTLIB\n",
    "plt.scatter(df.column1, df.column2)\n",
    "plt.xlabel('Column1')\n",
    "plt.ylabel('Column2')\n",
    "plt.title('Title');\n",
    "# other kinds: 'plot', 'bar', 'barh', 'hist', 'boxplot', 'pie', 'scatter', 'loglog' => ex \"plt.hist()\"\n",
    "\n",
    "# For histograms, add \"bins=100\" option\n",
    "df.column.hist(bins=100)\n",
    "plt.hist(df.column, bins=100)\n",
    "\n",
    "# If you need to compare two or more distributions depending on the value of a column, df.condition:\n",
    "sns.histplot(data, x='column1', hue='condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Grid of subplots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a 3x4 (3 height, 4 width) grid of plots\n",
    "fig, ax = plt.subplots(3,4, figsize=(14,12),sharey = True, sharex = True)\n",
    "column = list(df.columns)\n",
    "for i in range(3):\n",
    "for j in range(4):\n",
    "# This is the trick: i*width + j\n",
    "ax[i,j].hist(df[column[i*4+j]], bins=100)\n",
    "ax[i,j].set_title(column[i*4+j])\n",
    "plt.show()\n",
    "# Give the whole grid of subplots a title if you want to using this\n",
    "fig.suptitle('Total title', fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 -  Logplots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram with y-value in log\n",
    "plt.hist(df.column, bins=100, log=True)\n",
    "# Log scale x and y\n",
    "plt.loglog(df.column1, df.column2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 - Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Heatmaps with two values ----\n",
    "\n",
    "# To get the number of occurences for each (column1,column2) pair\n",
    "df_heatmap = pd.crosstab(df.column1, df.column2)\n",
    "sns.heatmap(df_heatmap, annot=True) # too edit scale: #, vmin = 0, vmax = 20);\n",
    "\n",
    "# ---- Heatmaps with three values ----\n",
    "# The color will represent the third variable (\"values\")\n",
    "df_heatmap = pd.crosstab(df.column1, df.column2, values=df.column3, margins=False, aggfunc='sum')\n",
    "sns.heatmap(df_heatmap, annot=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 - Plots with errorbars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNS barplots have errorbars directly computed\n",
    "sns.barplot(x=\"column1\", y=\"column2\", data=df)\n",
    "# You might want to adjust the limits of the y-axis to see if the errorbars overlap or not.\n",
    "# Otherwise use a CI, obtained by bootstraping for example (see tutorial 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 - Boxplotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(by='categdata', column='age', figsize = [5, 5], grid=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 - grouped bar plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding a new column \n",
    "df = df.assign(col3= np.where((df['col1'] == 0) & (df['col2'] == 0), 1, 0))\n",
    "ethnicity = df.groupby(['categcol'])[['col1','col2','col3']].sum()\n",
    "\n",
    "## put in relative terms \n",
    "ethnicity = ethnicity.div(ethnicity.sum(axis=1), axis=0)\n",
    "\n",
    "pl = ethnicity.plot(kind='bar', figsize=[7,5], rot=0)\n",
    "pl.set_title('ethnicity')\n",
    "pl.set_ylabel('participants')\n",
    "pl.set_xlabel('treatment')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Handling Data <a id=\"4\"> </a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Basic statistical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy way to get all at once: count, mean, std, min, max, 25%, 50%, 75%\n",
    "df.describe()\n",
    "# If you want something specific\n",
    "df.column_name.mean()\n",
    "df.column_name.median()\n",
    "df.column_name.std()\n",
    "df.column_name.sum()\n",
    "df.column_name.min()\n",
    "df.column_name.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 - distribution tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does the data come from a normal distrbution?\n",
    "diagnostic.kstest_normal(df.column1.values, dist='norm')\n",
    "# Does the data come form an exponential distribution?\n",
    "diagnostic.kstest_normal(df.column1.values, dist='exp')\n",
    "# Returns: (ksstat, pvalue).\n",
    "# If p-value < 0.05 => Reject Null hypothesis that it comes from a normal distribution => not normal dist.\n",
    "# If p-value > 0.05 => Do not reject null hypothesis.\n",
    "# Run this if you want more information on outputs/inputs:\n",
    "diagnostic.kstest_normal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 - Correlation tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NORMAL DISTIBUTION ASSUMPTION \n",
    "stats.pearsonr(df.column1, df.column2)\n",
    "# Returns: (r, p-value)\n",
    "# If p-value < 0.05 => Reject null hypothesis that the variables are uncorrelated => they are correlated\n",
    "# If p-value > 0.05 => Do not reject null hypothesis that the two variables are uncorrelated\n",
    "# r can be -1 and 1. The larger abs(r), the larger the correlation. (r<0 => as x increases, y decreases).\n",
    "stats.pearsonr?\n",
    "\n",
    "## NOT NECESSARY NORMALLY DISTRIBUTED \n",
    "\n",
    "stats.spearmanr(df.column1, df.column2)\n",
    "# Returns: (correlation, p-value)\n",
    "# Same interpretation as Pearson correlation\n",
    "stats.spearmanr?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 - Hypothesis testing (T-test)\n",
    " Two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. Assumes that the populations\n",
    "have identical variances by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(df.column1, df.column2)\n",
    "# Returns: (statistic, p-value)\n",
    "# The t-test quantifies difference between the means of the two samples.\n",
    "# Null hypothesis: the samples are drawn from populations with the same means\n",
    "# p-value < 0.05 => observation is unlikely to have occurred by chance => reject H => not same mean.\n",
    "# p-value > 0.05 => observation is not so unlikely to have occurred by chance => do not reject H.\n",
    "stats.ttest_ind?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - Linear regression <a id=\"5\"> </a> \n",
    "Equations are specified using patsy formula syntax. Important operators are:\n",
    "1. ~ : Separates the left-hand side and right-hand side of a formula.\n",
    "2. + : Creates a union of terms that are included in the model.\n",
    "3. : : Interaction term.\n",
    "4. * : a * b is short-hand for a + b + a:b , useful when you want to include all interactions between a set of variables.\n",
    "- Intercepts are added by default.\n",
    "- Categorical variables can be included directly by adding a term C(a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Basic Linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the model\n",
    "mod = smf.ols(formula='time ~ C(diabetes) + C(high_blood_pressure)', data=df)\n",
    "# Fit the model (finds the optimal coefficients, adding a random seed ensures consistency)\n",
    "np.random.seed(2)\n",
    "res = mod.fit()\n",
    "# Print the summary output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each predictor (e.g. C(diabities)) you get: coefficient, standard error of the coefficients, p-value, 95% confidence intervals. Significant\n",
    "predictor if p < 0.05.\\\n",
    "Interpretation: days at hospital = coeff(Intercept) + coeff(diabities) * diabetes(bool, 1 if yes) + coeff(high\n",
    "blood pressure) * high blood pressure(bool, 1 if yes) .\n",
    "- People who don't have diabetes nor high blood pressure stay at the hospital on average for coeff(intercept) days\n",
    "- People who have diabetes, but don't have blood pressure stay for coeff(intercept) + coeff(diabities) days\n",
    "- etc.\n",
    "### With interaction terms:\n",
    "Adding a*b adds terms a , b and a:b at once (you get a coefficient for each)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 - Logisitic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize CONTINUOUS features (get z-scores)\n",
    "df['cont_feature1'] = (df['cont_feature1'] - df['cont_feature1'].mean()) / df['cont_feature1'].std()\n",
    "df['cont_feature2'] = (df['cont_feature2'] - df['cont_feature2'].mean()) / df['cont_feature2'].std()\n",
    "# Perform logistic regression\n",
    "mod = smf.logit(formula='DEATH_EVENT ~ age + ... + C(diabities) + C(high_blood_pressure)', data=df)\n",
    "res = mod.fit()\n",
    "# Print the summary output\n",
    "print(res.summary())\n",
    "# To acces specific elements:\n",
    "variables = res.params.index\n",
    "coefficients = res.params.values\n",
    "p_values = res.pvalues\n",
    "standard_errors = res.bse.values\n",
    "CIs = res.conf_int() # confidence intervals!\n",
    "# Sort by coefficients:\n",
    "l1, l2, l3, l4 = zip(*sorted(zip(coefficients[1:], variables[1:], standard_errors[1:],"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Observartionnal studies <a id=\"6\"> </a> \n",
    "Equations are specified using patsy formula syntax. Important operators are:\n",
    "1. Standardize data --> continuous features \n",
    "2. Logisitc regression \n",
    "3. Propensity score matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Extract propensity score \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 - Standardize CONTINUOUS predictors (get z-scores)\n",
    "df['cont_feature1'] = (df['cont_feature1'] - df['cont_feature1'].mean()) / df['cont_feature1'].std()\n",
    "df['cont_feature2'] = (df['cont_feature2'] - df['cont_feature2'].mean()) / df['cont_feature2'].std()\n",
    "\n",
    "# 2 - Perform logistic regression\n",
    "mod = smf.logit(formula='treat ~ age + educ + C(black) + C(hispan)', data=df)\n",
    "res = mod.fit()\n",
    "\n",
    "# 3 - Extract estimated propensity scores (add to new column)\n",
    "df['Propensity_score'] = res.predict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Matching\n",
    "### Match subjects into pairs (1 treated, 1 control), with minimal propensity score difference\n",
    "\n",
    "We will create a network (using networkx library) with an edge between all instances. The library has a function that maximizes the sum of\n",
    "weights between pairs. All we need to do is to make sure we have large weights between two instances that have a small difference in\n",
    "propensity scores.\n",
    "\n",
    "We can use similarity: $$ similarity(x,y) = 1 - | propensity\\_score(x) - propensity\\_score(y) |$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute similarity score\n",
    "def get_similarity(propensity_score1, propensity_score2):\n",
    "    return 1-np.abs(propensity_score1-propensity_score2)\n",
    "\n",
    "# Separate the treatment and control groups\n",
    "treatment_df = df[df['treat'] == 1]\n",
    "control_df = df[df['treat'] == 0]\n",
    "# Create an empty undirected graph\n",
    "G = nx.Graph()\n",
    "# Loop through all the pairs (1 control, 1 treatment) of instances\n",
    "for control_id, control_row in control_df.iterrows():\n",
    "for treatment_id, treatment_row in treatment_df.iterrows():\n",
    "# Calculate the similarity between the two instances (=nodes)\n",
    "similarity = get_similarity(control_row['Propensity_score'], treatment_row['Propensity_score'])\n",
    "# Add an edge between the two instances (=nodes) weighted by the similarity between them\n",
    "G.add_weighted_edges_from([(control_id, treatment_id, similarity)])\n",
    "# Get the maximum weight (=max similarity!) matching on the generated graph\n",
    "matching = nx.max_weight_matching(G)\n",
    "# Get the new dataset with the matched pairs\n",
    "matched = [i[0] for i in list(matching)] + [i[1] for i in list(matching)]\n",
    "final_df = df.iloc[matched]\n",
    "# Now you can see how the results (ex plot) in the dataframe are more coherent!\n",
    "# They should get rid of the effect of the covariates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 - Supervised Learning <a id=\"7\"> </a> \n",
    "### __Supervized__: Input samples (X,y) . Learn f such that y=f(X) to evaluate the y of new samples X\n",
    "- Continuous y : regression\n",
    "- Discrete y : classification\n",
    "\n",
    "## 6.1 - Supervized learning --> Regression --> __Linear__ Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing feature (input) columns, get X\n",
    "feature_cols = ['input_column1', 'input_column2']\n",
    "X = df[feature_cols]\n",
    "# Choosing output column, y\n",
    "y = df.output_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 : Perform a linear regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Perform linear regression\n",
    "lin_reg = LinearRegression() # INITIALIZE REGRESSION\n",
    "lin_reg.fit(X,y) # FIT\n",
    "# Get the obtained beta coefficients\n",
    "for f in range(len(feature_cols)):\n",
    "    print(\"{0} * {1} + \".format(lin_reg.coef_[f], feature_cols[f]))\n",
    "print(lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, predicting the sales (y) with inputs (X): advertising on TV, Radio, Newspaper.\n",
    "This prints the following (intercept is $\\beta_0$, the other $\\beta_i$ are coefficients):\n",
    "$$y = \\beta_0 + \\beta_1 \\times TV + \\beta_2 \\times\n",
    "radio + \\beta_3 \\times newspaper$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 : Measure performance (MSE) using cross validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each entry of y_pred is a prediction obtained by cross-validation\n",
    "y_pred = cross_val_predict(lin_reg, X, y, cv=5)\n",
    "# Get MSE (mean square error) between prediction and actual\n",
    "mean_squared_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 : Regularization \n",
    "In our dataset, we might have many records, but let us imagine that we had much fewer records ?\n",
    "\n",
    "**Problem**: The model remembers the training records (overfitting).\n",
    "\n",
    "**Solution**: Regularization\n",
    "\n",
    "Regularization refers to methods that help to reduce overfitting. Let's try Ridge Regression, which puts a penalty on large weights $\\beta_i$ and forces them to be smaller in magnitude. This reduces the complexity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=5)\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "predicted_r = cross_val_predict(ridge, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 - Supervized learning --> Regression --> __Logistic__ Regression\n",
    "__Important__: For logistic regression, features must be either discrete or standardized\n",
    "You can therefore onehot encode or label column values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Prepare X and y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing feature (input) columns\n",
    "feature_cols = ['input_column1', 'input_column2']\n",
    "# Convert cathegorical values into dummy variables:\n",
    "X = pd.get_dummies(df[feature_cols])\n",
    "# You can also do this by simply labelling ('replace' is an easy way to do it)\n",
    "# Choosing output column (y)\n",
    "y = df.output_column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 : Perform a logistic regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Perform logistic regression\n",
    "logistic = LogisticRegression(solver='lbfgs') # INITIALIZE REGRESSION\n",
    "logistic.fit(X, y)                            # FIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Measure performance (Precision and Recall) using cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Precision and Recall\n",
    "precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")\n",
    "print(\"Precision: %0.2f (+/- %0.2f)\" % (precision.mean(), precision.std() * 2))\n",
    "recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")\n",
    "print(\"Recall: %0.2f (+/- %0.2f)\" % (recall.mean(), recall.std() * 2))\n",
    "# Plot ROC curve: See tutorial 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 - Supervized learning --> Classification --> __K-NN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# Example k-NN for K neighbours\n",
    "classifier = KNeighborsClassifier(15) # INITIALIZE CLASSIFIER\n",
    "classifier.fit(X_train, y_train) # FIT\n",
    "y_pred = classifier.predict(X_test) # PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 - Supervized learning → Classification → Random forests\n",
    "Important: In X and y, use dummies for non numerical variables (onehot encode or just label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# Example Random Forest with K trees\n",
    "classifier = RandomForestClassifier(max_depth=3, random_state=0, n_estimators=K) # INITIALIZE CLASSIFIER\n",
    "classifier.fit(X_train, y_train) # FIT\n",
    "y_pred = classifier.predict(X_test) # PREDICT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 - Applied Machine Learning <a id=\"8\"> </a> \n",
    "## 7.1 - Splitting the data into training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.model_selection import train_test_split\n",
    "# For a 70-30 split (70% train, 30% test), with random state 123\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
    "\n",
    "## can be done this way \n",
    "def split_set(data_to_split, ratio=0.8):\n",
    "    mask = np.random.rand(len(data_to_split)) < ratio\n",
    "    return [data_to_split[mask].reset_index(drop=True), data_to_split[~mask].reset_index(drop=True)]\n",
    "\n",
    "[train, test] = split_set(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 - create dummies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['col1','col2',..]\n",
    "train_categorical = pd.get_dummies(train, columns=categorical_columns)\n",
    "train_categorical.columns\n",
    "\n",
    "# Make sure we use only the features available in the training set\n",
    "test_categorical = pd.get_dummies(test, columns=categorical_columns)[train_categorical.columns]\n",
    "\n",
    "\n",
    "train_label=train_categorical.label\n",
    "train_features = train_categorical.drop('label', axis=1)\n",
    "print('Length of the train dataset : {}'.format(len(train)))\n",
    "\n",
    "test_label=test_categorical.label\n",
    "test_features = test_categorical.drop('label', axis=1)\n",
    "print('Length of the test dataset : {}'.format(len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardization \n",
    "means = train_features.mean()\n",
    "stddevs = train_features.std()\n",
    "\n",
    "train_features_std = pd.DataFrame()\n",
    "for c in train_features.columns:\n",
    "    train_features_std[c] = (train_features[c]-means[c])/stddevs[c]\n",
    "\n",
    "# Use the mean and stddev of the training set\n",
    "test_features_std = pd.DataFrame()\n",
    "for c in test_features.columns:\n",
    "    test_features_std[c] = (test_features[c]-means[c])/stddevs[c]\n",
    "\n",
    "train_features_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 - Computing the confusion matrix and recall, precision, F-score\n",
    "\n",
    "Confusion matrix: $\\begin{pmatrix}\n",
    "TP & FP \\\\\n",
    "FN & TN \\\\\n",
    "\\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: X_train and X_test should be standardized before this step\n",
    "logistic = LogisticRegression(solver='lbfgs') # INITIALIZE REGRESSION\n",
    "logistic.fit(X_train,y_train) # FIT\n",
    "y_pred = clf.predict(X_test) # PREDIT\n",
    "\n",
    "# For the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# For recall, precision, F-score\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">another way of doing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_confusion_matrix(true_label, prediction_proba, decision_threshold=0.5): \n",
    "    \n",
    "    predict_label = (prediction_proba[:,1]>decision_threshold).astype(int)   \n",
    "                                                                                                                       \n",
    "    TP = np.sum(np.logical_and(predict_label==1, true_label==1))\n",
    "    TN = np.sum(np.logical_and(predict_label==0, true_label==0))\n",
    "    FP = np.sum(np.logical_and(predict_label==1, true_label==0))\n",
    "    FN = np.sum(np.logical_and(predict_label==0, true_label==1))\n",
    "    \n",
    "    confusion_matrix = np.asarray([[TP, FP],\n",
    "                                    [FN, TN]])\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix):\n",
    "    [[TP, FP],[FN, TN]] = confusion_matrix\n",
    "    label = np.asarray([['TP {}'.format(TP), 'FP {}'.format(FP)],\n",
    "                        ['FN {}'.format(FN), 'TN {}'.format(TN)]])\n",
    "    \n",
    "    df_cm = pd.DataFrame(confusion_matrix, index=['Yes', 'No'], columns=['Positive', 'Negative']) \n",
    "    \n",
    "    return sn.heatmap(df_cm, cmap='YlOrRd', annot=label, annot_kws={\"size\": 16}, cbar=False, fmt='')\n",
    "\n",
    "\n",
    "def compute_all_score(confusion_matrix, t=0.5):\n",
    "    [[TP, FP],[FN, TN]] = confusion_matrix.astype(float)\n",
    "    \n",
    "    accuracy =  (TP+TN)/np.sum(confusion_matrix)\n",
    "    \n",
    "    precision_positive = TP/(TP+FP) if (TP+FP) !=0 else np.nan\n",
    "    precision_negative = TN/(TN+FN) if (TN+FN) !=0 else np.nan\n",
    "    \n",
    "    recall_positive = TP/(TP+FN) if (TP+FN) !=0 else np.nan\n",
    "    recall_negative = TN/(TN+FP) if (TN+FP) !=0 else np.nan\n",
    "\n",
    "    F1_score_positive = 2 *(precision_positive*recall_positive)/(precision_positive+recall_positive) if (precision_positive+recall_positive) !=0 else np.nan\n",
    "    F1_score_negative = 2 *(precision_negative*recall_negative)/(precision_negative+recall_negative) if (precision_negative+recall_negative) !=0 else np.nan\n",
    "\n",
    "    return [t, accuracy, precision_positive, recall_positive, F1_score_positive, precision_negative, recall_negative, F1_score_negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_proba = logistic.predict_proba(X_test_std) # PREDICT PROBABILITY\n",
    "# function in the next cell\n",
    "confusion_matrix = compute_confusion_matrix(y_test, prediction_proba, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 - Unsupervized learning <a id=\"9\"> </a> \n",
    "__Unsupervized__: Input samples X. Learn f such that y = f(X) is a “simpler” representation.\n",
    "- Discrete y : __clustering__\n",
    "- Continuous y : __dimensionality reduction__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 - Unsupervized Learning - __Clustering__ with K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.cluster import KMeans\n",
    "# Example K-means for 5 clusters\n",
    "kmean = KMeans(n_clusters=5, random_state=42).fit(X)\n",
    "# Plot the resulting scatter (color shows the clusters)\n",
    "plt.scatter(X[:,0], X[:,1], c=kmean.labels_)\n",
    "# Plot the centroid of each cluster\n",
    "for c in kmean.cluster_centers_:\n",
    "plt.scatter(c[0], c[1], marker=\"o\", color=\"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Select the right K__\n",
    "\n",
    "1. Option 1: Look at __silhouette score__:\n",
    "Find the K with the desired tradeoff between the number of clusters and cohesion/separation.\n",
    "To get the curve of the silhouette score, we cluster the data with different values of K and plot the resulting values\n",
    "(for the plot: x=K, y=Silhouette score).\n",
    "Choose K that maximizes silhouette score.\n",
    "\n",
    "2. Option 2: __elbow method__:\n",
    "Find the \"elbow\" in the curve of the Sum of Squared Errors\n",
    "To get the curve for elbow method, compute the SSE (sum of squared errors) for different values of K and plot the values\n",
    "(for the plot: x=K, y=SSE).\n",
    "Choose K that minimizes SSE.\n",
    "See below (end of part 8.3) for __K-means with high dimensional data__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 - Unsupervized learning - __Clustering__ with __DBSCAN__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.cluster import DBSCAN\n",
    "# Example DBSCAN with eps=0.13\n",
    "labels = DBSCAN(eps=0.13).fit_predict(X)\n",
    "# Remember to play around with the eps value, it is the radius of the spheres\n",
    "# eps grows => more points are density reachable => less clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 - Unsupervized learning - __Dimensionality Reduction__\n",
    "Want to reduce dimensions without losing too much information\n",
    "\n",
    "### Option1: __t-SNE__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.manifold import TSNE\n",
    "# Context: X10d was a 10-dimensional set of data points\n",
    "# Reduces X10d to 2-dimensions using t-SNE and call it X_reduced_tsne\n",
    "X_reduced_tsne = TSNE(n_components=2, random_state=0).fit_transform(X10d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### option 2 : __PCA__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import library\n",
    "from sklearn.decomposition import PCA\n",
    "# Context: X10d was a 10-dimensional set of data points\n",
    "# Reduce X10d to 2-dimensions using PCA and call it X_reduced_pca\n",
    "X_reduced_pca = PCA(n_components=2).fit(X10d).transform(X10d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use __K-means clustering on high dimensional data__, you have to perform the K-means on the original high-dimensional (ex: 10-D) data\n",
    "(X10d) and then plot the reduced-dimension (2-D) data (X_reduced_tsne or X_reduced_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9 - Handling text\n",
    "## 9.1 - Basic prossessing of text data\n",
    "### Loading text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11 - Handling Networks <a id=\"12\"> </a> \n",
    "## 11.1 - create a graph manually \n",
    "We will be using NetworkX library ( import networkx as nx ), and several other libraries after so just copy paste all libraries from import\n",
    "list at the beginning of this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTED graph\n",
    "G = nx.DiGraph()\n",
    "# UNDIRECTED graph\n",
    "G = nx.Graph()\n",
    "# Prints graph infomartion \n",
    "print(nx.info(G))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add nodes and edges "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add nodes\n",
    "G.add_node(1)\n",
    "# Add several nodes at once\n",
    "G.add_nodes_from(range(2,9))\n",
    "# Add edges\n",
    "G.add_edge(1,2)\n",
    "# Add several edges at once\n",
    "edges = [(2,3), (1,3), (4,1), (4,5)]\n",
    "G.add_edges_from(edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot + information graph/network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots the network\n",
    "nx.draw_spring(G, with_labels=True, alpha=0.8)\n",
    "# Plot a subnetwork\n",
    "subgraph_Alex = G.subgraph(['Alexander']+list(G.neighbors('Alexander')))\n",
    "# Just the same as plotting G but replace by the subnetworks name\n",
    "nx.draw_spring(subgraph_Alex, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Degree distribution P(k): probabilitiy that a randomly chosen node has a degree k. Normalized histogram: $P(k)=\\frac{N_k}{N}$. Consider\n",
    "plotting in log-log, especially for power laws!\n",
    "- NB: If you just need the degree of one node use G.degree(node) .\n",
    "For in-degree: G.in_degree(node) , for out-degree: G.out_degree(node)\n",
    "- __Function for plotting the degree distribution:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for plotting the degree distribution of a Graph\n",
    "def plot_degree_distribution(G):\n",
    "    degrees = {}\n",
    "    for node in G.nodes():\n",
    "        degree = G.degree(node)\n",
    "        if degree not in degrees:\n",
    "            degrees[degree] = 0\n",
    "        degrees[degree] += 1\n",
    "    sorted_degree = sorted(degrees.items())\n",
    "    deg = [k for (k,v) in sorted_degree]\n",
    "    cnt = [v for (k,v) in sorted_degree]\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.bar(deg, cnt, width=0.80, color='b')\n",
    "    plt.title(\"Degree Distribution\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xlabel(\"Degree\")\n",
    "    ax.set_xticks([d+0.05 for d in deg])\n",
    "    ax.set_xticklabels(deg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for printing various graph properties\n",
    "def describe_graph(G):\n",
    "    print(nx.info(G))\n",
    "    if nx.is_connected(G):\n",
    "        print(\"Avg. Shortest Path Length: %.4f\" %nx.average_shortest_path_length(G))\n",
    "        print(\"Diameter: %.4f\" %nx.diameter(G)) # Longest shortest path\n",
    "    else:\n",
    "        print(\"Graph is not connected\")\n",
    "        print(\"Diameter and Avg shortest path length are not defined!\")\n",
    "    print(\"Sparsity: %.4f\" %nx.density(G))  # #edges/#edges-complete-graph\n",
    "    # #closed-triplets(3*#triangles)/#all-triplets\n",
    "    print(\"Global clustering coefficient aka Transitivity: %.4f\" %nx.transitivity(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for visualizing the graph\n",
    "def visualize_graph(G, with_labels=True, k=None, alpha=1.0, node_shape='o'):\n",
    "    #nx.draw_spring(G, with_labels=with_labels, alpha = alpha)\n",
    "    pos = nx.spring_layout(G, k=k)\n",
    "    if with_labels:\n",
    "        lab = nx.draw_networkx_labels(G, pos, labels=dict([(n, n) for n in G.nodes()]))\n",
    "    ec = nx.draw_networkx_edges(G, pos, alpha=alpha)\n",
    "    nc = nx.draw_networkx_nodes(G, pos, nodelist=G.nodes(), node_color='g', node_shape=node_shape)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.2 - Networks from pandas dataframes\n",
    "Creating a graph from a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a dataframe with edgelist\n",
    "df_edges = pd.read_csv('data/edgelist.csv')\n",
    "# In df, 'Source' and 'Target' are columns that give the edges\n",
    "# For directed graph\n",
    "G = nx.from_pandas_edgelist(df_edges, 'Source', 'Target', edge_attr=None, create_using=nx.DiGraph())\n",
    "# For undirected graph\n",
    "G = nx.from_pandas_edgelist(df_edges, 'Source', 'Target', edge_attr=None, create_using=nx.Graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node attributes are in the node list dataframe\n",
    "df_nodes = pd.read_csv('data/nodelist.csv')\n",
    "# Add these attributes to their corresponding node in the graph\n",
    "nx.set_node_attributes(G, df_nodes['Gender'].to_dict(), 'Gender')\n",
    "nx.set_node_attributes(G, df_nodes['Birthdate'].to_dict(), 'Birthdate')\n",
    "# This way you can access one node's attributes very easily:\n",
    "G.nodes['Guillaume Ryelandt'] # (here the index column in the df_nodes was the names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'Role': 'EPFL student',\n",
    "'Gender': 'male',\n",
    "'Birthdate': 1999}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.3 - describing the networks \n",
    "### Sparsity\n",
    "Sparsity of a graph with $n$ nodes is defined as the number of edges over the maximumum number of edges.\n",
    "- A directed graph can have at most $n(n-1)$ edges\n",
    "- An undirected graph can have at most $n(n-1)/2$ edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.density(G)\n",
    "print(\"The sparsity of the graph is: {}\".format(nx.density(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transitivity \n",
    "(a.k.a. Global clustering coefficient)\n",
    "It is the overall probability for the network to have adjacent nodes interconnected, thus revealing the existence of tightly connected\n",
    "communities (or clusters). \"A friend of my friend is my friend\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL clustering coefficient (for the whole graph)\n",
    "nx.transitivity(G)\n",
    "print(\"The transitivity of the graph is: {}\".format(nx.transitivity(G)))\n",
    "print(\"The global clustering coefficient of the graph is: {}\".format(nx.transitivity(G)))\n",
    "# Clustering coefficient (between two nodes)\n",
    "nx.clustering(G, ['Alexander', 'John']))\n",
    "print(\"The clustering coefficient is: {}\".format(nx.clustering(G), ['Alexander', 'John']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.4 - is the graph connected ? \n",
    "The graph is connected if __there is a path from any point to any other point in the graph__. No seperate \"communities\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if nx.is_connected(G):\n",
    "    print(\"The network is connected.\")\n",
    "else:\n",
    "    print(\"The network is not connected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for connected graphs \n",
    "We can define:\n",
    "- Average shortest path length\n",
    "- Longest shortest path (Diameter)\n",
    "- Shortest path between two nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average shortest path length:\n",
    "nx.average_shortest_path_length(G)\n",
    "print(\"The average shortest path length is {}\".format(nx.average_shortest_path_length(G)))\n",
    "# Longest shortest path length (a.k.a diameter):\n",
    "print(\"The longest shortest path length (diameter) is {}\".format(nx.diameter(G)))\n",
    "# Shortest path between two nodes:\n",
    "tom_bob_path = nx.shortest_path(G, source=\"Thomas\", target=\"Bob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for non-connected graphs \n",
    "we can look at the number of components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of connected components of G\n",
    "comp = list(nx.connected_components(G))\n",
    "print('The graph contains {} connected components'.format(len(comp)))\n",
    "\n",
    "# Get the largest component\n",
    "largest_comp = max(comp, key=len)\n",
    "H = G.subgraph(list(largest_comp))\n",
    "print('The largest component contains {} nodes'.format(len(largest_comp)))\n",
    "print(\"The largest component: {}\".format(nx.info(H)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### strong and weak connection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strongly connected graph\n",
    "nx.is_strongly_connected(G)\n",
    "# Weakly connected graph\n",
    "nx.is_weakly_connected(G)\n",
    "# There are many built-in functions in NetworkX, look them up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.6 - Measuring importance of a node\n",
    "\n",
    "# Note: this is an undirected graph. If you were to have a **directed** one, use separate metrics for **indegree** and **outdegree**.\n",
    "### Degree centrality\n",
    "Many neighbours = important node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(G.degree(G.nodes()))\n",
    "sorted_degree = sorted(G.items(), key=itemgetter(1), reverse=True)\n",
    "# Get the top 5 most popular nodes\n",
    "for quaker, degree in sorted_degree[:5]:\n",
    "print(quaker, 'who is', G.nodes[quaker]['Role'], 'knows', degree, 'people')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Katz Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(G.degree(G.nodes()))\n",
    "# Compute katz centrality\n",
    "katz = nx.katz_centrality(G)\n",
    "# Add it to node attributes and sort\n",
    "nx.set_node_attributes(G, katz, 'katz')\n",
    "sorted_katz = sorted(katz.items(), key=itemgetter(1), reverse=True)\n",
    "# Get the top 5 most popular nodes\n",
    "for quaker, katzc in sorted_katz[:5]:\n",
    "    print(quaker, 'who is', G.nodes[quaker]['Role'], 'has katz-centrality: %.3f' %katzc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Betweenness centrality :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute betweenness centrality\n",
    "betweenness = nx.betweenness_centrality(G)\n",
    "# Add it to node attributes and sort\n",
    "nx.set_node_attributes(G, betweenness, 'betweenness')\n",
    "sorted_betweenness = sorted(betweenness.items(), key=itemgetter(1), reverse=True)\n",
    "# Get the top 5 most popular nodes\n",
    "for quaker, bw in sorted_betweenness[:5]:\n",
    "    print(quaker, 'who is', G.nodes[quaker]['Role'], 'has betweeness: %.3f' %bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### show the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similar pattern\n",
    "list_nodes =list(quakerG.nodes())\n",
    "list_nodes.reverse()   # for showing the nodes with high betweeness centrality \n",
    "pos = nx.spring_layout(quakerG)\n",
    "ec = nx.draw_networkx_edges(quakerG, pos, alpha=0.1)\n",
    "nc = nx.draw_networkx_nodes(quakerG, pos, nodelist=list_nodes, node_color=[quakerG.nodes[n][\"betweenness\"] for n in list_nodes], \n",
    "                            alpha=0.8, node_shape = '.')\n",
    "plt.colorbar(nc)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.7 - Community detection \n",
    "### Girvan Newman \\\n",
    "Idea: Edges with high betweeness centrality separate communities. Algorithm starts with the entire graph and then it iteratively removes the\n",
    "edge with the highest betweeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = girvan_newman(G)\n",
    "iteration = 0\n",
    "for communities in itertools.islice(comp, 4):\n",
    "iteration +=1\n",
    "print('Iteration', iteration)\n",
    "print(tuple(sorted(c) for c in communities))\n",
    "visualize_graph(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Louvain method \n",
    "\n",
    "Idea: Proceeds the other way around: initially every node is considered as a community. The communities are traversed, and for each\n",
    "community it is tested whether joining it to a neighboring community gives us a better clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2451269364.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/1x/fjwg5qd1275c9755rz7f68pw0000gn/T/ipykernel_10145/2451269364.py\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    G.nodes[n][\"louvain\"] = partition[n]\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "partition = community_louvain.best_partition(G)\n",
    "# Add it as an attribute to the nodes\n",
    "for n in G.nodes:\n",
    "G.nodes[n][\"louvain\"] = partition[n]\n",
    "# Plot it out\n",
    "pos = nx.spring_layout(G,k=0.2)\n",
    "ec = nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "nc = nx.draw_networkx_nodes(G, pos, nodelist=G.nodes(),\n",
    "node_color=[G.nodes[n][\"louvain\"] for n in G.nodes],\n",
    "node_size=100, cmap=plt.cm.jet)\n",
    "plt.show()\n",
    "# ------------------------------ Take a look at specific cluster ------------------------------\n",
    "cluster_James = partition['James Nayler']\n",
    "# Take all the nodes that belong to James' cluster\n",
    "members_cluster = [q for q in G.nodes if partition[q] == cluster_James]\n",
    "\n",
    "# Get information about them\n",
    "for quaker in members_cluster:\n",
    "print(quaker, 'who is', G.nodes[quaker]['Role'], 'and died in ', G.nodes[quaker]['Deathdate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11.8 - Homophily in network\n",
    "### Influence: I copy eating behavior of those around me\n",
    "### Homophily: people with similar eating behavior prone to become friends\n",
    "\n",
    "How likely is it that two quakers who have the same attribute are linked?\n",
    "\n",
    "Try to measure the similarity of connections in the graph with respect to a given attribute.   \n",
    "*Intuition: Like correlation, but translated to graphs.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For categorical attributes\n",
    "nx.attribute_assortativity_coefficient(G, 'Gender')\n",
    "# For numerical attributes, values must be integers\n",
    "nx.numeric_assortativity_coefficient(G, 'Deathdate')\n",
    "# If this value is high, means there is some form of gender homophily in the network.\n",
    "# People who are the same gender tend to belong to the same network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 13 - problems <a id=\"14\"> </a> \n",
    "## TODO \n",
    "> read before the exam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOMEWORK 1\n",
    "    - question 1.2 : count duplicates in a column "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.8.15 ('ADA')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "100px",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "33a84d476f468faad60f82f72ffeabf7137776612d1aa6170926cd686dcd2eb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
